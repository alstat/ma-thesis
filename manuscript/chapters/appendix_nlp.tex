\chapter{Background on Natural Language Processing}\label{ch:nlp}
This chapter discusses the concepts of Natural Language Processing in the context of analyzing the Qur'\=an.

\section{Text Analytics}
Technically, any information can be regarded as data, whether it be image, audio, video, or text, and many more forms. All of these raw data, however, need to be translated into numbers, and this is true for texts as well. There are several ways to translate texts into numbers; the easiest way perhaps is to map letters into numbers. For example, $a$ would be 1, $b$ would be 2, and so on. This simple assignment, while easy to implement, disregards much of the information or representation in the texts. Therefore, when mapping raw data not in numeric form, the assignment must be logical. One needs to ensure that the transformation from its raw form accounts for the characteristics of the texts. For example, any language has words with synonyms and antonyms. Mapping these to numbers should account for these relations as well. Since synonyms depict words related to a given word, their numeric form should be close to each other. If "beautiful" corresponds to 132, then "attractive" should have a numeric mapping close to 132 since the words "beautiful" and "attractive" are considered synonymous or related. On the other hand, "ugly," which is an antonym of "beautiful," should have a corresponding numeric form far from 132 to capture the opposition between the two in numerical form. The next section will discuss the formal mathematical definition for this concept of representing texts.

\section{Tokenization}\label{sec:text_tokenization}
When analyzing natural language presented as text, it is important to understand that, like human beings digesting a paragraph word by word, mathematical computations require texts to be broken down into pieces before they can be fed into formulas. Disintegrating texts into smaller units is called \textit{tokenization}, and these units are called \textit{tokens}.

\begin{exmp}
    Consider the \textit{basmala}, tokenize it into words.\\
    \textit{Solution:} Let $X=\text{"\arb[fullvoc]{bismi 'l-lahi 'l-ra.hmAni 'l-rahImi}"}$, if $f(\cdot;s)$ is the tokenizer function such that $s$ is the type of delimiter, then
    \begin{equation}
        f(X;s="\;\,")=[\text{\arb[fullvoc]{bismi}},\text{\arb[fullvoc]{'l-lahi}},\text{\arb[fullvoc]{'l-ra.hmAni}},\text{\arb[fullvoc]{'l-rahImi}}]^{\text{T}}
    \end{equation}
\end{exmp}

\section{Embeddings}
Embeddings is the technical term for translating words into numbers; it is the process of 'embedding' words into a Euclidean vector space. Let's understand this piece by piece. A logical approach to translating a word into numbers would be to use one-hot encoding, which is a process of representing a word as a sparse vector with only one non-zero entry. Let's say the language contains only three words: 'hi', 'hello', and 'yes'. Then, the one-hot encoding for this would be:
\begin{align}
    \text{hi}&=[1,0,0]\\
    \text{hello}&=[0,1,0]\\
    \text{yes}&=[0,0,1]
\end{align}

Notice that each word has a unique identity in its vector encoding. Now, what if the language contains one million words? Then each word will have 999,999 zeros in its vector encoding and only one non-zero entry. If a vector contains many more zeros than non-zeros, it is referred to as \textit{sparse}. In this case, it is indeed very sparse. Furthermore, applying such methodology is not flexible, since it is tied to a fixed number of words, which is problematic even for Arabic due to its complex morphology. It is preferable to have a dense vector that also captures the semantics of the words.

Ideally, these very high-dimensional vectors should be condensed into a much lower dimension to avoid the \textit{curse of dimensionality}, a case where vectors which should be close to each other become more distinct due to the high dimensionality. Apart from this, the embedding in the vector space should also capture semantic proximity, meaning words that are semantically similar should be close to each other in the embedding space. There are two main approaches: \textit{Term Frequency - Inverse Document Frequency} and \textit{Word Embeddings}.

\subsection{Term Frequency - Inverse Document Frequency}\label{sec:tf-idf}
The TF-IDF or Term Frequency - Inverse Document Frequency is an NLP method for embedding words. It is composed of two statistics: the TF and the IDF. The following are the formal definitions of both statistics.

\begin{defnx}[Term Frequency]
Let $d$ be the document, and let $i$ be a term or word in the document, then the \textit{term frequency} of the $i$th term in the document $d$ is denoted by $\operatorname{tf}(i,d)$ with the following formula: $\operatorname{tf}(i,d)=\displaystyle\frac{|i|}{\sum_{\forall i'\in d}|i'|}$, where $i'$ is any other term other than $i$.
\end{defnx}

\begin{defnx}[Inverse Document Frequency]
Let $N$ be the total number of documents $\mathscr{D}$ in the corpus, and let $i$ be the term in $d$ such that $d\in \mathscr{D}$, then the \textit{inverse document frequency} is defined by
\begin{equation}
    \operatorname{idf}(i,d)=\log\frac{N}{|\{d:d\in \mathscr{D}\;\text{and}\;i\in d\}|}
\end{equation}
\end{defnx}

\subsection{Word Embeddings}\label{sec:word-embeddings}
TF-IDF has several limitations. As discussed earlier, each word will have its own dimension, resulting in embeddings with very high dimensionality. Furthermore, while TF-IDF can create pairs of words in its bag-of-words approach to add some context to a given word, it still cannot capture the full context of a sentence. Lastly, TF-IDF cannot handle out-of-vocabulary words without recomputing all calculations to account for the new word.

Given these limitations of TF-IDF, \textit{word embeddings} based on Neural Networks have become popular since they address all of TF-IDF's limitations. First, these word embeddings are dense and lower in dimension compared to TF-IDF embeddings. Second, these embeddings capture the semantic relationships and similarities between words. Advanced models like Bidirectional Encoder Representation from Transformer (BERT) and Generative Pre-Trained Transformers (GPT) can generate contextualized embeddings, meaning the same word can have different representations depending on its context within a sentence. Both BERT and GPT are discussed in the next appendix. Third, these embeddings have the flexibility to handle out-of-vocabulary words. Lastly, embeddings pre-trained on large corpora can be fine-tuned for specific tasks.

In summary, while TF-IDF is a straightforward and interpretable method for measuring term importance, it falls short in capturing the semantic richness and contextual nuances of language. Neural network-based word embeddings address these limitations by providing dense, semantically rich, and context-aware representations of words, making them more powerful and versatile for a wide range of natural language processing tasks.